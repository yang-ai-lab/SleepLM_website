<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SleepLM: Natural-Language Intelligence for Human Sleep.">
  <meta name="keywords" content="SleepLM, Sleep, LLM, Multimodal, Healthcare">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SleepLM: Natural-Language Intelligence for Human Sleep</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="#">
            SleepLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SleepLM: Natural-Language Intelligence for Human Sleep</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Zongzhe Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Zitao Shuai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Eideen Mozaffari</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Ravi Shankar Aysola</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Rajesh Kumar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuzhe Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YourRepo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/YourDataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Placeholder for Teaser Video/Image -->
      <div style="text-align: center;">
         <img src="./static/images/interpolate_start.jpg" alt="SleepLM Teaser" style="width: 100%; max-height: 400px; object-fit: cover;">
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">SleepLM</span> is a family of foundation models that bridge natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <strong>SleepLM</strong>, a family of sleep-language foundation models that enable human sleep alignment,
            interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based
            sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to
            describe, query, or generalize to novel sleep phenomena.
          </p>
          <p>
            SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To
            support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the
            curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more
            than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines
            contrastive alignment, caption generation, and signal reconstruction to better capture physiological
            fidelity and cross-modal interactions.
          </p>
          <p>
            Extensive experiments on real-world sleep understanding tasks
            verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal
            retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including
            language-guided event localization, targeted insight generation, and zero-shot generalization to
            unseen tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Motivation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Humans move between two distinct states: the waking life, structured by perception and language; and sleep, expressed through dense and continuous physiology.
            Making sense of sleep requires a mapping from physiology to language.
          </p>
          <p>
            Current computational methods are predominantly discriminative and confined to <strong>closed label spaces</strong> (e.g., sleep stages or events).
            They lack the capacity for <strong>open-ended description or reasoning</strong>. SleepLM addresses this by learning to translate high-frequency sleep physiology into actionable language descriptions.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->

    <!-- Model Architecture. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Architecture: ReCoCa</h2>
        <div class="content has-text-justified">
          <p>
            SleepLM is built on the <strong>Reconstructive Contrastive Captioner (ReCoCa)</strong> framework, designed to learn joint representations of sleep PSG and text.
            The architecture consists of three key components:
          </p>
          <ul>
            <li>
              <strong>Channel-Specific Sleep Encoder:</strong>
              Captures the unique morphology of different sensor channels (EEG, EOG, etc.) using channel-independent patch embedding followed by interleaved temporal-attention and channel-attention blocks.
            </li>
            <li>
              <strong>Signal Reconstruction Decoder:</strong>
              A lightweight decoder that predicts original signal patches from encoder latents. This acts as a regularizer, ensuring the model retains physiological fidelity that might be lost if only training on text alignment.
            </li>
            <li>
              <strong>Modality-Conditioned Text Decoder:</strong>
              Generates descriptive captions. It uses a learnable token <code>[m]</code> to condition generation on specific physiological systems (Brain, Respiratory, Cardiac, Somatic), enabling controllable output.
            </li>
          </ul>
          <p>
            The model is trained with a composite objective function:
            <code>L_total = λ_con * L_con + λ_rec * L_rec + λ_cap * L_cap</code>, combining contrastive alignment, signal reconstruction, and caption generation.
          </p>
          <div style="text-align: center; margin-top: 20px;">
             <!-- Placeholder for Method Figure -->
             <img src="./static/images/interpolate_end.jpg" alt="SleepLM Architecture" style="width: 80%; border: 1px solid #ddd; border-radius: 5px;">
             <p class="is-size-6">Figure 1: The ReCoCa Architecture (Placeholder for Figure 3 from paper)</p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Model Architecture. -->

    <!-- Capabilities. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Capabilities</h2>
        <div class="content has-text-justified">
          <p>
            SleepLM demonstrates versatile capabilities beyond standard classification:
          </p>
          <ul>
            <li><strong>Zero-Shot Recognition:</strong> Outperforms proprietary LLMs (like Gemini 2.5 Pro, DeepSeek-R1) and fine-tuned VLMs on sleep staging and event localization without any task-specific training.</li>
            <li><strong>Cross-Modal Retrieval:</strong> Enables precise searching of sleep segments using natural language queries (e.g., "rapid eye movement with irregular breathing") and vice-versa.</li>
            <li><strong>Unseen Concept Generalization:</strong> Can identify clinical events not seen during training (e.g., "Mixed Apnea") by leveraging semantic relationships in the learned embedding space.</li>
            <li><strong>Few-Shot Learning:</strong> Achieves high accuracy (0.90 AUC) with as few as 50 samples per class, demonstrating superior data efficiency compared to self-supervised baselines.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Capabilities. -->

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Benchmark Highlights</h3>
        <div class="content has-text-justified">
          <p>
            SleepLM is evaluated on four complementary axes of sleep understanding and generalization.
          </p>
          <ul>
            <li><b>Zero-shot classification:</b> strong performance without task-specific finetuning, demonstrating that the learned signal–text alignment transfers to standard sleep staging and event-related recognition settings.</li>
            <li><b>Cross-modal retrieval:</b> supports both text→signal and signal→text retrieval, enabling natural-language search over PSG segments and interpretation of retrieved physiology with matched captions.</li>
            <li><b>Few-shot learning:</b> achieves high data efficiency in low-label regimes, where simple adapters/linear probes on top of pretrained representations perform competitively with stronger task-specific baselines.</li>
            <li><b>Generalization to unseen concepts:</b> remains robust when evaluating on held-out concepts and datasets, indicating that the model captures reusable physiological semantics rather than memorizing a closed label set.</li>
          </ul>
        </div>

        <h3 class="title is-4">Qualitative Insights</h3>
        <div class="content has-text-justified">
          <p>
            We additionally visualize generation quality and representation structure using two qualitative analyses from the paper.
          </p>
        </div>

        <div class="columns is-variable is-4">
          <div class="column">
            <figure class="image">
              <img src="./static/images/figure5_generation_quality.png" alt="Figure 5: caption generation quality">
            </figure>
            <p class="is-size-7 has-text-centered">
              Figure 5: Caption generation quality (replace with the exported figure from the paper).
            </p>
          </div>
          <div class="column">
            <figure class="image">
              <img src="./static/images/figure7_embedding_continuity.png" alt="Figure 7: embedding space continuity">
            </figure>
            <p class="is-size-7 has-text-centered">
              Figure 7: Embedding space continuity (replace with the exported figure from the paper).
            </p>
          </div>
        </div>

        <div class="content has-text-justified">
          <p class="is-size-7">
            Note: The quantitative benchmarks above are summarized in text because the corresponding plots are not included on this page yet.
            If you later export the benchmark figures, they can be added under “Benchmark Highlights” in the same style as the qualitative figures.
          </p>
        </div>

      </div>
    </div>
    <!--/ Results. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2026sleeplm,
  title={SleepLM: Natural-Language Intelligence for Human Sleep},
  author={Xu, Zongzhe and Shuai, Zitao and Mozaffari, Eideen and Aysola, Ravi Shankar and Kumar, Rajesh and Yang, Yuzhe},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/YourRepo" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
