<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SleepLM: Natural-Language Intelligence for Human Sleep.">
  <meta name="keywords" content="SleepLM, Sleep, LLM, Multimodal, Healthcare">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SleepLM: Natural-Language Intelligence for Human Sleep</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/sleeplm_favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="#">
            SleepLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SleepLM: Natural-Language Intelligence for Human Sleep</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Zongzhe Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Zitao Shuai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Eideen Mozaffari</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Ravi Shankar Aysola</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Rajesh Kumar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuzhe Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YourRepo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/YourDataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Placeholder for Teaser Video/Image -->
      <div style="text-align: center;">
         <img src="./static/images/figure1.png" alt="SleepLM Teaser" style="width: 100%; max-height: 400px; object-fit: cover;">
      </div>
      <h2 class="subtitle has-text-centered">
        SleepLM is a family of foundation models that bridge natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <strong>SleepLM</strong>, a family of sleep-language foundation models that enable human sleep alignment,
            interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based
            sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to
            describe, query, or generalize to novel sleep phenomena.
          </p>
          <p>
            SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To
            support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the
            curation of the <strong>first</strong> large-scale sleep-text dataset, comprising over 100K hours of data from more
            than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines
            contrastive alignment, caption generation, and signal reconstruction to better capture physiological
            fidelity and cross-modal interactions.
          </p>
          <p>
            Extensive experiments on real-world sleep understanding tasks
            verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal
            retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including
            language-guided event localization, targeted insight generation, and zero-shot generalization to
            unseen tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Motivation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Humans move between two distinct states: the waking life, structured by perception and language; and sleep, expressed through dense and continuous physiology.
            Making sense of sleep requires a mapping from physiology to language.
          </p>
          <p>
            Current computational methods are predominantly discriminative and confined to closed label spaces (e.g., sleep stages or events).
            They lack the capacity for open-ended description. SleepLM aims to bridge this gap by learning a mapping between PSG and language at scale, enabling interactive and open-ended sleep analysis.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->

    <!-- Model Architecture. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Architecture: ReCoCa</h2>
        <div class="content has-text-justified">
          <p>
            SleepLM is built on the <strong>Reconstructive Contrastive Captioner (ReCoCa)</strong> framework, designed to learn joint representations of sleep PSG and text.
            The architecture consists of three key components:
          </p>
          <ul>
            <li>
              <strong>Channel-Specific Sleep Encoder:</strong>
              Captures the unique morphology of different sensor channels (EEG, EOG, etc.) using channel-independent patch embedding followed by interleaved temporal-attention and channel-attention blocks.
            </li>
            <li>
              <strong>Signal Reconstruction Decoder:</strong>
              A lightweight decoder that predicts original signal patches from encoder latents. This acts as a regularizer, ensuring the model retains physiological fidelity that might be lost if only training on text alignment.
            </li>
            <li>
              <strong>Modality-Conditioned Text Decoder:</strong>
              Generates descriptive captions. It uses a learnable token <code>[m]</code> to condition generation on specific physiological systems (Brain, Respiratory, Cardiac, Somatic), enabling controllable output.
            </li>
          </ul>
          <p>
            The model is trained with a composite objective function:
            <code>L_total = λ_con * L_con + λ_rec * L_rec + λ_cap * L_cap</code>, combining contrastive alignment, signal reconstruction, and caption generation.
          </p>
          <div style="text-align: center; margin-top: 60px; margin-bottom: 20px;">
             <img src="./static/images/model_figure.png" alt="SleepLM Architecture" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
             <p class="is-size-5">The ReCoCa Architecture</p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Model Architecture. -->

    <!-- Capabilities. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Capabilities</h2>
        <div class="content has-text-justified">
          <p>
            SleepLM demonstrates versatile capabilities beyond standard classification:
          </p>
          <ul>
            <li><strong>Zero-Shot Recognition:</strong> Outperforms proprietary LLMs (like Gemini 2.5 Pro, DeepSeek-R1) and fine-tuned VLMs on sleep staging and event localization without any task-specific training.</li>
            <li><strong>Cross-Modal Retrieval:</strong> Enables precise searching of sleep segments using natural language queries (e.g., "rapid eye movement with irregular breathing") and vice-versa.</li>
            <li><strong>Unseen Concept Generalization:</strong> Can identify clinical events not seen during training (e.g., "Mixed Apnea") by leveraging semantic relationships in the learned embedding space.</li>
            <li><strong>Few-Shot Learning:</strong> Achieves high accuracy (0.90 AUC) with as few as 50 samples per class, demonstrating superior data efficiency compared to self-supervised baselines.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Capabilities. -->

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Benchmark Highlights</h3>
        <div class="content has-text-justified">
          <p>
            SleepLM is evaluated on four complementary axes of sleep understanding and generalization. It outperforms a comprehensive set of baseline models including proprietary LLMs (like Gemini 2.5 Pro, DeepSeek-R1) and finetuned VLMs.
          </p>
          <p>
          <strong>Zero-shot classification:</strong> strong performance without task-specific finetuning, demonstrating that the learned signal and text alignment transfers to standard sleep staging and event related recognition settings.</li>
          </p>
          <p>
          <strong>Cross-modal retrieval:</strong> supports both text to signal and signal to text retrieval, enabling natural language search over PSG segments and interpretation of retrieved physiology with matched captions.</li>
          </p>
          <p>
          <strong>Few-shot learning:</strong> achieves high data efficiency in low label regimes, where simple linear probes on top of pretrained representations perform competitively with state-of-the-art SSL baselines.</li>
          </p>
          <p>
          <strong>Generalization to unseen concepts:</strong> remains robust when evaluating on heldout concepts and datasets, indicating that the model captures reusable physiological semantics rather than memorizing a closed label set.</li>
          </p>
          
        </div>

        <h3 class="title is-4">Qualitative Insights</h3>
        <div class="content has-text-justified">
          <p>
            In this section, we include two qualitative views that make it easier to understand what the model learns and
            where its strengths come from. We also demonstrate other intriguing capabilities of SleepLM including precise targeted generation,
            localization sensitivity to event segmentation, scaling behavior, full night index aggregation, etc. For more details, please refer to the paper.
          </p>
          <p>
            <strong>Caption generation quality</strong>: SleepLM produces clinically consistent descriptions that reflect both high level sleep state
            and finegrained localized events, while a strong general-purpose LLM baseline may introduce incorrect
            associations or miss event localization tied to the underlying signal evidence.
          </p>
          <p>
            <strong>Embedding-space continuity</strong>: For a fixed PSG query, retrieved captions
            form a smooth semantic gradient. The most similar results describe physiologically matching states, and progressively less similar
            results shift toward increasingly different states. This supports the view that embedding distance correlates with physiological similarity,
            enabling meaningful comparison by semantic proximity.
          </p>
        </div>

        <div style="text-align: center; margin-top: 60px; margin-bottom: 20px;">
          <img src="./static/images/generation_quality.png" alt="Caption generation quality" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
          <p class="is-size-5 has-text-centered">
            Caption generation quality
          </p>
        </div>
        <div style="text-align: center; margin-top: 60px; margin-bottom: 20px;">
          <img src="./static/images/retrieval_qualitative.png" alt="Embedding space continuity" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
          <p class="is-size-5 has-text-centered">
            Embedding space continuity
          </p>
        </div>

      </div>
    </div>
    <!--/ Results. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2026sleeplm,
  title={SleepLM: Natural-Language Intelligence for Human Sleep},
  author={Xu, Zongzhe and Shuai, Zitao and Mozaffari, Eideen and Aysola, Ravi Shankar and Kumar, Rajesh and Yang, Yuzhe},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/YourRepo" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>